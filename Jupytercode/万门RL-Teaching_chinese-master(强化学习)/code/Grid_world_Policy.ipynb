{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# environment: grids with size m*n; goal / orginary grid / windy grid / cliff grid / blocking grid / start point (Can be random).\n",
    "# other variation: \n",
    "# task: can be temporal discounting (R(goal)=0, R(orginary) =-1), or not (R(goal)=1, others =0).\n",
    "# different learning algorithm: TD (evaluation with rand walk / e-greedy; SARSA; Q-learning)\n",
    "# GUI to be added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.debugger import set_trace\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# grid configuration\\ngsize = [3,3]\\ns0=[1,0] # initial state. just don't start at goal state\\ngw = np.zeros([gsize[0],gsize[1]]) # 0 is orginary block\\ngw[0,0]= 1 # set goals\\ngw[2,2] = 1 \\nacts = ['u','d','l','r']\\n# action and transition matrix\\ndef state_act(state,action,gsize):\\n    # action is a character of either u,d,l,r (up,down,left,right)\\n    # state is a 1*2 tuple, marking the current position\\n    newstate = state[:]\\n    if action == 'u' or action == 0:\\n        newstate[1]=max(0,state[1]-1)\\n    elif action == 'd' or action == 1:\\n        newstate[1]=min(gsize[1]-1,state[1]+1)\\n    elif action == 'l' or action == 2:\\n        newstate[0]=max(0,state[0]-1)\\n    elif action == 'r' or action == 3:\\n        newstate[0]=min(gsize[0]-1,state[0]+1)\\n    else:\\n        raise ValueError('action not valid')\\n    \\n    if gw[newstate[0],newstate[1]] == -1: # means it's blocked, so remain unmoved\\n        newestate = state[:]\\n        \\n    return newstate    \\n   \\n# reward setup\\ndef reward(state, gw):\\n    # state represents the current position; gw is the setting of grid world\\n    if gw[state[0],state[1]] == 1:\\n        R = 0\\n    else:\\n        R = -1 # with temporal discount\\n    return R\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Environment setup: Sutton book example 4.1\n",
    "\"\"\"\n",
    "# grid configuration\n",
    "gsize = [3,3]\n",
    "s0=[1,0] # initial state. just don't start at goal state\n",
    "gw = np.zeros([gsize[0],gsize[1]]) # 0 is orginary block\n",
    "gw[0,0]= 1 # set goals\n",
    "gw[2,2] = 1 \n",
    "acts = ['u','d','l','r']\n",
    "# action and transition matrix\n",
    "def state_act(state,action,gsize):\n",
    "    # action is a character of either u,d,l,r (up,down,left,right)\n",
    "    # state is a 1*2 tuple, marking the current position\n",
    "    newstate = state[:]\n",
    "    if action == 'u' or action == 0:\n",
    "        newstate[1]=max(0,state[1]-1)\n",
    "    elif action == 'd' or action == 1:\n",
    "        newstate[1]=min(gsize[1]-1,state[1]+1)\n",
    "    elif action == 'l' or action == 2:\n",
    "        newstate[0]=max(0,state[0]-1)\n",
    "    elif action == 'r' or action == 3:\n",
    "        newstate[0]=min(gsize[0]-1,state[0]+1)\n",
    "    else:\n",
    "        raise ValueError('action not valid')\n",
    "    \n",
    "    if gw[newstate[0],newstate[1]] == -1: # means it's blocked, so remain unmoved\n",
    "        newestate = state[:]\n",
    "        \n",
    "    return newstate    \n",
    "   \n",
    "# reward setup\n",
    "def reward(state, gw):\n",
    "    # state represents the current position; gw is the setting of grid world\n",
    "    if gw[state[0],state[1]] == 1:\n",
    "        R = 0\n",
    "    else:\n",
    "        R = -1 # with temporal discount\n",
    "    return R\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Environment setup: Sutton book example 6.6 cliff walk\n",
    "# grid configuration\n",
    "gsize = [12,4]\n",
    "s0=[0,0] # initial state\n",
    "gw = np.zeros([gsize[0],gsize[1]]) # 0 is orginary block\n",
    "gw[-1,0]= 1 # set goals\n",
    "gw[1:-2,0]= -100 # cliff\n",
    "acts = ['u','d','l','r']\n",
    "# action and transition matrix\n",
    "def state_act(state,action,gsize):\n",
    "    # action is a character of either u,d,l,r (up,down,left,right)\n",
    "    # state is a 1*2 tuple, marking the current position\n",
    "    newstate = state[:]\n",
    "    if action == 'u' or action == 0:\n",
    "        newstate[1]=max(0,state[1]-1)\n",
    "    elif action == 'd' or action == 1:\n",
    "        newstate[1]=min(gsize[1]-1,state[1]+1)\n",
    "    elif action == 'l' or action == 2:\n",
    "        newstate[0]=max(0,state[0]-1)\n",
    "    elif action == 'r' or action == 3:\n",
    "        newstate[0]=min(gsize[0]-1,state[0]+1)\n",
    "    else:\n",
    "        raise ValueError('action not valid')\n",
    "    \n",
    "    if gw[newstate[0],newstate[1]] == -100: # fall into the cliff? return to the initial state\n",
    "        newestate = [0,0]\n",
    "        \n",
    "    return newstate    \n",
    "   \n",
    "# reward setup\n",
    "def reward(state, gw):\n",
    "    # state represents the current position; gw is the setting of grid world\n",
    "    if gw[state[0],state[1]] == 1: # goal\n",
    "        R = 0\n",
    "    elif gw[state[0],state[1]] == -100: # cliff        \n",
    "        R = -100 # with temporal discount\n",
    "    else:\n",
    "        R = -1\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Learner setup\n",
    "A = .5 # learning rate\n",
    "gamma = 1 # no temporal discount for future state\n",
    "def e_greedy(state, Q): #  e-greedy\n",
    "    e = 0.1 # setting recommended by example 6.5 & 6.6\n",
    "    if np.random.rand(1) < e:\n",
    "        action=np.random.randint(len(acts))\n",
    "    else:\n",
    "        Q_now = Q[state[0]][state[1]]\n",
    "        #action = np.argmax(Q_now)\n",
    "        allmax = [i for i, j in enumerate(Q_now) if j == max(Q_now)] # find all actions of largest Qs\n",
    "        action = allmax[np.random.randint(len(allmax))] # randomly select one if there's tie; otherwise\n",
    "\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 112.    51.4   38.4   27.8   22.7   22.3   15.9   17.1   16.    16.2\n",
      "   15.8   14.    15.    15.9   14.4   14.7   14.5   15.8   14.1   14.6]\n",
      "[-904.   -81.1  -58.2  -47.6  -62.3  -32.2  -85.2 -145.8  -55.6  -45.9\n",
      "  -25.7  -14.   -84.3  -75.3  -24.3  -64.2  -24.4 -164.3  -24.   -64.1]\n"
     ]
    }
   ],
   "source": [
    "# start learning\n",
    "Nepis = 200 # total episode num\n",
    "Q = np.zeros([gsize[0],gsize[1],len(acts)])\n",
    "\n",
    "isQLearning=True\n",
    "isSARSA=False\n",
    "steps = np.zeros(Nepis)\n",
    "r_epis = np.zeros(Nepis)\n",
    "for k in range(Nepis):\n",
    "    s = s0\n",
    "    nstep = 0\n",
    "    tot_r = 0\n",
    "    while gw[s[0],s[1]] != 1:\n",
    "        a=e_greedy(s,Q)\n",
    "        s_new = state_act(s,a,gsize)\n",
    "        if isSARSA == True:\n",
    "            a_new = e_greedy(s_new,Q) # use the same policy as current state\n",
    "        elif isQLearning == True:\n",
    "            a_new = np.argmax(Q[s_new[0]][s_new[1]]) # use the best possible action        \n",
    "        nstep = nstep + 1\n",
    "        # print(s,s_new,acts[a])            \n",
    "        pred_err = reward(s,gw) + gamma*Q[s_new[0],s_new[1],a_new] - Q[s[0],s[1],a]\n",
    "        tot_r = tot_r+reward(s,gw)\n",
    "        Q[s[0],s[1],a] = Q[s[0],s[1],a] + A * pred_err\n",
    "        s = s_new\n",
    "        \n",
    "    steps[k] = nstep\n",
    "    r_epis[k] = tot_r\n",
    "    #print(tot_r)\n",
    "    \n",
    "# show the decrease of steps per episode -- average over every 10 episodes\n",
    "print(np.mean(np.reshape(steps,(int(Nepis/10),10)),axis=1)) \n",
    "print(np.mean(np.reshape(r_epis,(int(Nepis/10),10)),axis=1)) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
